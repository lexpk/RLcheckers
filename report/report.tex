\documentclass[a4paper,12pt]{article}

\title{A Reinforcement Learning Agent for Checkers\\based on Transformers, MCTS and TD($\lambda$)}
\author{Alexander Pluska}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{ wasysym }
\usepackage{ stmaryrd }
\usepackage{ mathpartir }
\usepackage{bussproofs}
\usepackage{enumerate}
\usepackage{tikz-qtree}
\usepackage{stackengine}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pgfplots}
\usepackage{xfp}

	
\usepackage{hyperref}% http://ctan.org/pkg/hyperref
\usepackage{cleveref}% http://ctan.org/pkg/cleveref

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{definition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		The goal of this project is to implement a reinforcement learning agent for checkers. The used algorithm is inspired by early versions of AlphaGo~\cite{silver2016mastering}, although heavy modifications had to be undertaken to accommodate for lack of computational resources. We demonstrate that the agent learns, but is still far from being competitive with a checkers engine.
	\end{abstract}
	
	\section{Introduction}
	
	Having followed the development of AlphaZero in awe, I thought it would be interesting to mimic some of their ideas and make my own reinforcement learning agent for a board game. To achieve convergence AlphaZero required many hundreds of thousands simulations~\cite{silver2018general}.
	Deepmind's incredible success with AlphaZero relied on massive parallelization, incredible computing resources and certainly also a much more efficient implementation than is feasible for me. Therefore I decided to make a number of adjustments:
	\begin{itemize}
		\item Instead of Chess or Go I decided to tackle Checkers, a game that is still complex enough to be interesting but simple enough to be implemented in a reasonable time frame.
		\item Instead of two networks, a distinct policy and a valuation network, I decided to use only one valuation network, which would completely determine the policy function via tree search and negamax.
	\end{itemize}
	
	In early versions of AlphaGo the agent was bootstrapped by supervised learning on expert moves~\cite{silver2016mastering}. However, for checkers a database of such expert moves is not readily available and creating it is beyond the scope of the project, so instead the value network is bootstrapped on random positions with material count as a target.
	
	To ensure termination the game is augmented with an additional rule: The game ends immediately in a draw when a position is repeated. Clearly if there exists a winning strategy from a position then there exists one that avoids repetitions, so an optimal policy for the augmented game is also optimal for usual checkers. Otherwise the rules are those of American checkers, i.e. Kings can move and capture diagonally forward and backward, but only one step at a time.


	\section{The Agent}
	
	\subsection{Evaluation Function}

	At the core of the agent is a evaluation function that is used to evaluate positions. The evaluation function is a neural network that takes a board position as input and outputs a value between $0$ and $1$ that is interpreted as the probability of the current player winning from the position. In our case we use a relatively small transformer~\cite{viswanathan2017attention} with 16 layers, 4 heads and an embedding dimension of 128. The outputs of the transformer are passed through a linear layer, averaged and then passed through a sigmoid function to obtain the final evaluation. The transformer contains an embedding for each pair of square and piece, i.e. 32 embeddings for dark men, 32 for dark kings, 32 for light men, 32 for light kings and 32 for empty squares. During evaluation, a position is first converted into 32 embeddings, one for each square, and then passed through the transformer.

	\subsection{Monte-Carlo Tree Search}

	Given the evaluation function, the agents policy is determined by a Monte-Carlo-like tree search. The search tree is initialized with the current position as the single root node with $0$ expansions and then repeatedly expanded as follows:
	\begin{itemize}
		\item If a node with $0$ expansions is to be expanded, add a child to the node for each legal move in its position and expand each of them until no more captures are possible. Then evaluate the leaves with the neural net and update the evaluation of its parents recursively via negamax, i.e. 
		$$ev(\text{node}) = \max\{1 - ev(\text{child})\:|\:\text{child}\in\text{children}\}.$$
		\item If a node $n$ already has expansions, expand the child $c$ for which the value of
		$$1-ev(c) + \sqrt{2\cdot\frac{\log(\text{expansions}(n))}{\text{expansions}(c)}}$$
		is maximal, ensuring a trade-off between exploration and exploitation.
	\end{itemize}
	Expansions are performed until a time limit is reached or a forced win is found. The move chosen is the one associated with the child with minimal evaluation, i.e. minimal predicted value for the opponent.

	\subsection{Learning}

	Learning is performed in two stages:
	
	First the transformer is initialized to approximate the material value of positions. This is done by supervised learning on 10 million randomly generated positions.
	
	Finally the agent learns through self-play. During each stage the agent plays itself from the initial position + 4 random moves. After 128 games the agent is trained on the resulting positions. The target value for the positions is determined by TD($\lambda$) with $\lambda = 0.9$. I.e. if the game consists of positions $p_0,p_1,\ldots,p_n$ and ended in a win for the current player, then the target value $v(p_i)$ for $p_i$ is given inductively by
	$$v(p_i) = \begin{cases}
	1 & \text{if } i = n\\
	\lambda\cdot v(p_{i+1}) + (1-\lambda)\cdot v(p_{i}) & \text{otherwise}
	\end{cases}$$
	Self-play is performed on the CPU while learning is performed on a GPU.
	
	\section{Evaluation}
	
	For evaluation agents from multiple stages of learning were pitted against each other in a round-robin tournament. These were the contestants:
	\begin{itemize}
		\item A baseline agent playing random moves (r)
		\item The agent after only the supervised learning ($0$)
		\item The agents after $1-7$ stages of self-play ($1-7$)
	\end{itemize}

	The results of the tournament can be seen below. The numbers indicate score out of 8 games (win = 1, draw = 0, loss = -1).

	\newcommand{\loss}[1]{\cellcolor{yellow!\fpeval{100*#1/8}}-#1}
	\newcommand{\win}[1]{\cellcolor{blue!\fpeval{50*#1/8}}#1}

	\definecolor{green}{rgb}{0.0, 0.5, 0.0}
	\begin{center}
		\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|}
			& r & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & total \\\hline
			r & \cellcolor[gray]{0.8}x&\loss{8}&\loss{2}&\loss{8}&\loss{5}&\loss{7}&\loss{4}&\loss{6}&\loss{7}&-47\\\hline
			0 & \win{8} & \cellcolor[gray]{0.8}x& \win{6}& \win{5}& \win{1}&\win{6}&\win{5}&\win{6}&\win{2}&39\\\hline
			1 &\win2& \loss 6 & \cellcolor[gray]{0.8}x&\loss2&\loss7&\win1&\loss4&\loss1&\loss6&-23\\\hline
			2 &\win8&\loss5&\win2& \cellcolor[gray]{0.8}x&\loss8&\win2&\loss2&\win2&\loss5&-6\\\hline
			3 &\win5&\loss1&\win7&\win8& \cellcolor[gray]{0.8}x&\win 7&\win 4&\win 7&\win 2&39\\\hline
			4 &\win7&\loss6&\loss1&\loss2&\loss7& \cellcolor[gray]{0.8}x&\loss 3&\win 0&\loss 4&-16\\\hline
			5 &\win4 &\loss5&\win4&\win2&\loss4&\win3& \cellcolor[gray]{0.8}x & \win5&\loss1&8\\\hline
			6 &\win6&\loss6&\win1 &\loss2 & \loss7 &\win0 & \loss5 & \cellcolor[gray]{0.8}x&\loss7&-20\\\hline
			7 &\win7& \win3 & \loss2 & \win6 & \win5 & \loss2 & \win4 & \win7 & \cellcolor[gray]{0.8}x&25			
		\end{tabular}
		
	\end{center}
	
	\section{Conclusion}
	
	Looking at the tournament results there is a number of takeaways:
	\begin{itemize}
		\item There is definitely some kind of forgetting happening, as seen in stages 1 and 4. To alleviate this one could increase the number of games played during self-play or decrease the learning rate.
		\item The worst performing agent (besides random) is the one directly following the supervised learning on material estimation. This indicates that the policy obtained from TD($\lambda$) meaningfully differs from the one obtained from material estimation.
		\item The provided training regime was insufficient to outperform the baseline trained on material estimation. However, in stage 3 the agent was able learn a different policy that performer equally to the baseline, which is interesting.
		\item The biggest bottleneck for training was self-play, each stage taking about 2 hours for just 128 games. This could be alleviated by using a more efficient implementation and more computational resources.
		\item At least each stage was able to consistently beat a random player, which indicates that the implementation is correct and the agent is learning something.
	\end{itemize}
	
	
	\bibliographystyle{plain}
	\bibliography{report}
	
\end{document}